{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jpm_Copy of Word embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/james-monahan/Code-school-notebooks/blob/main/Week-14-nlp-regex/Word_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqw_NvWnNHq6"
      },
      "source": [
        "# Word embedding manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsBTr3zNPu4"
      },
      "source": [
        "## Load pre-trained embeddings\n",
        "\n",
        "You could train your own word embedding (using library like [gensim](https://radimrehurek.com/gensim/models/word2vec.html))  if you want, however you would need a lot of text and you would have to determine a ton of parameters (What is the size of your context, how big do you want your embedding, which algorithm to use, etc.).\n",
        "\n",
        "Why go through all that hassle when you can just use embeddings that specialist in the field already trained on huge corpus?\n",
        "\n",
        "[SpaCy](https://spacy.io/usage/models) is a library for NLP that provide such embeddings.\n",
        "\n",
        "### Run the code bellow :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3YRZwRkM98k"
      },
      "source": [
        "# Download the embeddings\n",
        "\n",
        "!python3 -m spacy download en_core_web_md\n",
        "\n",
        "# Load them\n",
        "\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2ie5NjIZ_uW"
      },
      "source": [
        "### Some optionnal information on this model \n",
        "\n",
        "The word embeddings of this model are of size 300 (a pretty standard size) and are trained using [GloVe](https://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/) algorithm. The model you loaded also come with other types of embeddings that may be useful for other NLP tasks (like Part Of speech vectors). \n",
        "\n",
        "There also exist a larger model with more words and models for other languages (see the SpaCy link)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7jXkmpINUkl"
      },
      "source": [
        "## Tokens embeddings and similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIxKNZZHflq4"
      },
      "source": [
        "Now that the model is loaded, we can give it a sentence and it will tokenise it and return a list of tokens with a number of attributes.\n",
        "\n",
        "Run the two following cells and try to understand them : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-FvGXNKN9Iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a129970e-6abc-450e-c021-840dbab1c5df"
      },
      "source": [
        "tokens = nlp(\"Hello, I'm a data analyst. aabbbb\")\n",
        "\n",
        "for t in tokens:\n",
        "    print(t.text, t.has_vector, t.vector_norm)\n",
        "\n",
        "# The attribute has_vector for \"aabbbb\" is False, it mean that no vector exist for this word in the model."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello True 5.586428\n",
            ", True 5.094723\n",
            "I True 6.4231944\n",
            "'m True 5.9417286\n",
            "a True 5.306696\n",
            "data True 7.1505103\n",
            "analyst True 7.489983\n",
            ". True 4.9316354\n",
            "aabbbb False 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hDZxEBlhKoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6902b96-799c-412c-8b3c-402c8c7ddf8b"
      },
      "source": [
        "print('Vector of \"' + tokens[0].text + '\" : \\n', tokens[0].vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vector of \"Hello\" : \n",
            " [ 0.25233    0.10176   -0.67485    0.21117    0.43492    0.16542\n",
            "  0.48261   -0.81222    0.041321   0.78502   -0.077857  -0.66324\n",
            "  0.1464    -0.29289   -0.25488    0.019293  -0.20265    0.98232\n",
            "  0.028312  -0.081276  -0.1214     0.13126   -0.17648    0.13556\n",
            " -0.16361   -0.22574    0.055006  -0.20308    0.20718    0.095785\n",
            "  0.22481    0.21537   -0.32982   -0.12241   -0.40031   -0.079381\n",
            " -0.19958   -0.015083  -0.079139  -0.18132    0.20681   -0.36196\n",
            " -0.30744   -0.24422   -0.23113    0.09798    0.1463    -0.062738\n",
            "  0.42934   -0.078038  -0.19627    0.65093   -0.22807   -0.30308\n",
            " -0.12483   -0.17568   -0.14651    0.15361   -0.29518    0.15099\n",
            " -0.51726   -0.033564  -0.23109   -0.7833     0.018029  -0.15719\n",
            "  0.02293    0.49639    0.029225   0.05669    0.14616   -0.19195\n",
            "  0.16244    0.23898    0.36431    0.45263    0.2456     0.23803\n",
            "  0.31399    0.3487    -0.035791   0.56108   -0.25345    0.051964\n",
            " -0.10618   -0.30962    1.0585    -0.42025    0.18216   -0.11256\n",
            "  0.40576    0.11784   -0.19705   -0.075292   0.080723  -0.02782\n",
            " -0.15617   -0.44681   -0.15165    0.1692     0.098255  -0.031894\n",
            "  0.087143   0.26082    0.002706   0.1319     0.34439   -0.37894\n",
            " -0.4114     0.081571  -0.11674   -0.43711    0.011144   0.099353\n",
            "  0.26612    0.40025    0.18895   -0.18438   -0.30355   -0.2725\n",
            "  0.22468   -0.40614    0.15618   -0.16043    0.47147    0.0080203\n",
            "  0.56858    0.21934   -0.11181    0.79925    0.10714   -0.50146\n",
            "  0.063593   0.069465   0.15292   -0.2747    -0.20989    0.20737\n",
            " -0.10681    0.40651   -2.6438    -0.31139   -0.32157   -0.26458\n",
            " -0.35625    0.070013  -0.18838    0.48773   -0.26167   -0.020805\n",
            "  0.17819    0.15758   -0.13752    0.056464   0.30766   -0.066136\n",
            "  0.4748    -0.27335    0.09732   -0.20832    0.0039332  0.346\n",
            " -0.08702   -0.54924   -0.18759   -0.17174    0.060324  -0.13521\n",
            "  0.10419    0.30165    0.05798    0.21872   -0.073594  -0.20423\n",
            " -0.25279   -0.10471   -0.32163    0.12525   -0.31281    0.0097207\n",
            " -0.26777   -0.61121   -0.11089   -0.13652    0.035135  -0.4939\n",
            "  0.084857  -0.15494   -0.063509  -0.23935    0.28272    0.10849\n",
            " -0.3365    -0.60764    0.38576   -0.0095438  0.17499   -0.52723\n",
            "  0.62211    0.19544   -0.48977    0.036582  -0.128     -0.016827\n",
            "  0.25647   -0.31698    0.48257   -0.14184    0.11046   -0.3098\n",
            " -0.63141   -0.37268    0.23183   -0.14268   -0.02341    0.022255\n",
            " -0.044662  -0.16404   -0.25848    0.1629     0.024751   0.23348\n",
            "  0.27933    0.38998   -0.058968   0.11355    0.15673    0.18583\n",
            " -0.19814   -0.48123   -0.035084   0.078458  -0.49833    0.10855\n",
            " -0.20133    0.05292   -0.11583   -0.16009    0.16768    0.42362\n",
            " -0.23106    0.082465   0.24296   -0.16786    0.0080409  0.085947\n",
            "  0.38033    0.072981   0.1633     0.24704   -0.11094    0.15115\n",
            " -0.22068   -0.061944  -0.037091  -0.087923  -0.23181    0.15035\n",
            " -0.19093   -0.19113   -0.11894    0.094908  -0.0043347  0.15362\n",
            " -0.41201   -0.3073     0.18375    0.40206   -0.0034793 -0.10917\n",
            " -0.69522    0.10161   -0.079256   0.40329    0.22285   -0.19374\n",
            " -0.13315    0.073231   0.099832   0.11685   -0.21643   -0.1108\n",
            "  0.10341    0.097286   0.11196   -0.3894    -0.0089363  0.28809\n",
            " -0.10792    0.028811   0.32545    0.26052   -0.038941   0.075204\n",
            "  0.46031   -0.06293    0.21661    0.17869   -0.51917    0.33591  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HYFTWYfgsIr"
      },
      "source": [
        "You can also get the similarity between two tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVrXPY9xgoWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22160623-2758-44f6-8d1c-235243066e44"
      },
      "source": [
        "tokens = nlp(\"dog cat banana\")\n",
        "\n",
        "for i in range(len(tokens)):\n",
        "    for j in range(i+1, len(tokens)):\n",
        "        print(tokens[i].text, tokens[j].text, tokens[i].similarity(tokens[j]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog cat 0.80168545\n",
            "dog banana 0.24327643\n",
            "cat banana 0.28154364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk4vnFT3kdJ8"
      },
      "source": [
        "**Warning** : You may find other pre-trained embeddings that you want to use or even train your owns with another library. All library has different methods, attributes and ways of handling embeddings, read the documentation and examples before using them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWxsDQZDN93H"
      },
      "source": [
        "# Sentence embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-J_6NjCmyzH"
      },
      "source": [
        "Now you know how to manipulate word embeddings, congratulation. \n",
        "So you have the sentence that you want to classify, and you have the embedding of each word of this sentence... Now what?\n",
        "\n",
        "Maybe you can concatenate all of these vectors and just give it to the classifier? \n",
        "\n",
        "Problems: \n",
        "\n",
        "- It would give a very very big vector. \n",
        "\n",
        "- It would be EXTREMELY sensible of the orders of the words \n",
        "\n",
        "- You would have to handle sentence having difference size with padding.\n",
        "\n",
        "In practice, state of the art model either train special sentence embeddings for their task or use special sequential neural network (RNN/LSTM). \n",
        "\n",
        "But we won't do that here (phew!). Actually just doing the average of the vectors works surprisingly well. And good news spacy comes with this functionality!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHtFgFAkOBpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2251a18-59a1-46ec-8c1f-c61dbbb626e9"
      },
      "source": [
        "tokens = nlp(\"Hello, I am a sentence.\")\n",
        "tokens.vector.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG1IN73AtJKs"
      },
      "source": [
        "You can also get sentences similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCD70Bf4tSYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3461bb-f6a7-457b-bf74-9026c057067b"
      },
      "source": [
        "tokens1 = nlp(\"Hello, I am a sentence.\")\n",
        "tokens2 = nlp(\"Hi, also some sort of phrase!\")\n",
        "tokens3 = nlp(\"This cat is cute.\")\n",
        "\n",
        "print(tokens1.similarity(tokens2))\n",
        "print(tokens1.similarity(tokens3))\n",
        "print(tokens2.similarity(tokens3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.832282210939598\n",
            "0.7502564755692778\n",
            "0.7618915522647609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix-ElKAogROu",
        "outputId": "0b215a43-3cbf-45a8-e6a6-c0fd8afb8e8e"
      },
      "source": [
        "tokens1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hello, I am a sentence."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KU8ZHKYvIYz"
      },
      "source": [
        "Just doing a mere average on untreated sentence actually have one problem: it gives to much weight to stop word or other very frequent and not important words. \n",
        "\n",
        "That is why you should delete the stop words like you did previously.\n",
        "\n",
        "Try to do it now and compute the embeddings for each treated sentences : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LLqsKXOd4hP"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('popular')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRP3vH7SpkdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d618a4-ede5-4f1e-82e4-468e3e79f900"
      },
      "source": [
        "stopwordsenglish  = nltk.corpus.stopwords.words(\"english\")\r\n",
        "'Hello,' in stopwordsenglish"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDIzh7pXd_nx",
        "outputId": "c9255404-5a9a-4748-e354-0514e84781d8"
      },
      "source": [
        "tokens1 = \"Hello, I am a sentence.\"\r\n",
        "tokens2 = \"Hi, also some sort of phrase!\"\r\n",
        "tokens3 = \"This cat is cute.\"\r\n",
        "\r\n",
        "#interesting if manually remove punc\r\n",
        "# tokens1 = \"Hello I am a sentence\"\r\n",
        "# tokens2 = \"Hi also some sort of phrase\"\r\n",
        "# tokens3 = \"This cat is cute\"\r\n",
        "\r\n",
        "tokens1 = [word for word in tokens1.split() if word.lower() not in stopwordsenglish]\r\n",
        "tokens2 = [word for word in tokens2.split() if word.lower() not in stopwordsenglish]\r\n",
        "tokens3 =  [word for word in tokens3.split() if word.lower() not in stopwordsenglish]\r\n",
        "\r\n",
        "tokens1 = nlp(\" \".join(tokens1))\r\n",
        "tokens2 = nlp(\" \".join(tokens2))\r\n",
        "tokens3 = nlp(\" \".join(tokens3))\r\n",
        "\r\n",
        "print(tokens1.similarity(tokens2))\r\n",
        "print(tokens1.similarity(tokens3))\r\n",
        "print(tokens2.similarity(tokens3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8174121063676709\n",
            "0.6202515567511576\n",
            "0.6381025221733543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckCwE7qUf8Qw",
        "outputId": "d6d7b0a2-70b8-4df2-985c-7c66ad2866c3"
      },
      "source": [
        "tokens1, tokens2, tokens3 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Hello, sentence., Hi, also sort phrase!, cat cute.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATk9SisbOBVy"
      },
      "source": [
        "# Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeCfPLpDOPFR"
      },
      "source": [
        "## The dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGq2dvArzv5O"
      },
      "source": [
        "### Run the code bellow :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO6c5rye0eH5"
      },
      "source": [
        "We won't use the twitter dataset that you already know because as strong as embeddings are they aren't great with unknown words/abreviation/emoji and the twitter dataset is full of them.\n",
        "\n",
        "We will instead use a dataset with review from Amazon, Yelp and IMDB. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjs4j_2zOpWW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "68be5e0e-3ff6-43c3-e926-7560ab9ca6cf"
      },
      "source": [
        "import pandas as pd\n",
        "df_source = pd.read_csv(\"https://raw.githubusercontent.com/CindyAloui/datasets_wcs/master/sentiment_dataset.csv\", usecols=(\"sentence\", \"sentiment\", \"source\"))\n",
        "df_source"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here i...</td>\n",
              "      <td>0</td>\n",
              "      <td>amazon_cells_labelled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1</td>\n",
              "      <td>amazon_cells_labelled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "      <td>amazon_cells_labelled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>0</td>\n",
              "      <td>amazon_cells_labelled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "      <td>amazon_cells_labelled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>I think food should have flavor and texture an...</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp_labelled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>Appetite instantly gone.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp_labelled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>Overall I was not impressed and would not go b...</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp_labelled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>The whole experience was underwhelming, and I ...</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp_labelled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp_labelled</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  ...                 source\n",
              "0     So there is no way for me to plug it in here i...  ...  amazon_cells_labelled\n",
              "1                           Good case, Excellent value.  ...  amazon_cells_labelled\n",
              "2                                Great for the jawbone.  ...  amazon_cells_labelled\n",
              "3     Tied to charger for conversations lasting more...  ...  amazon_cells_labelled\n",
              "4                                     The mic is great.  ...  amazon_cells_labelled\n",
              "...                                                 ...  ...                    ...\n",
              "2995  I think food should have flavor and texture an...  ...          yelp_labelled\n",
              "2996                           Appetite instantly gone.  ...          yelp_labelled\n",
              "2997  Overall I was not impressed and would not go b...  ...          yelp_labelled\n",
              "2998  The whole experience was underwhelming, and I ...  ...          yelp_labelled\n",
              "2999  Then, as if I hadn't wasted enough of my life ...  ...          yelp_labelled\n",
              "\n",
              "[3000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62avNnxSyFSH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "9a1ac226-a44c-40da-adaf-f905b7b92bfa"
      },
      "source": [
        "df_source.groupby(['source', 'sentiment']).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>source</th>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">amazon_cells_labelled</th>\n",
              "      <th>0</th>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">imdb_labelled</th>\n",
              "      <th>0</th>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">yelp_labelled</th>\n",
              "      <th>0</th>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 sentence\n",
              "source                sentiment          \n",
              "amazon_cells_labelled 0               500\n",
              "                      1               500\n",
              "imdb_labelled         0               500\n",
              "                      1               500\n",
              "yelp_labelled         0               500\n",
              "                      1               500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZuQ4rBaOSXQ"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL_BaWDO0VAp"
      },
      "source": [
        "Now you have all the elements to train a classifier for sentiment analysis using embeddings! A little reminder of the steps: \n",
        "\n",
        "- First take out the stop words so you won't have to do a weighted average. You can also lemmatize the text is you want but in this case it shouldn't have a big influence.\n",
        "\n",
        "- Then compute the sentence embeddings of the reviews. This is going to be our features.\n",
        "\n",
        "- Do a train test split.\n",
        "\n",
        "- Choose a type of classifier you want to use (for example a Logistic Regression).\n",
        "\n",
        "- Train and evaluate your classifier. \n",
        "\n",
        "You should be able to reach easily an accuracy of 80%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaKXyV5oO7Wp"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import nltk\r\n",
        "nltk.download('popular')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Z3Lq7xLKhn07",
        "outputId": "b4611bb1-683f-4705-bb9c-b2003a915996"
      },
      "source": [
        "df_source.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>source</th>\n",
              "      <th>no_stops</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>I think food should have flavor and texture an...</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp_labelled</td>\n",
              "      <td>think food flavor texture lacking.</td>\n",
              "      <td>[[-0.16743083, 0.19883184, 0.049234163, -0.275...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>Appetite instantly gone.</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp_labelled</td>\n",
              "      <td>Appetite instantly gone.</td>\n",
              "      <td>[[-0.07648475, 0.191955, -0.057646506, -0.2006...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>Overall I was not impressed and would not go b...</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp_labelled</td>\n",
              "      <td>Overall impressed would go back.</td>\n",
              "      <td>[[0.0021266676, 0.29672068, -0.16125, -0.01485...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>The whole experience was underwhelming, and I ...</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp_labelled</td>\n",
              "      <td>whole experience underwhelming, think we'll go...</td>\n",
              "      <td>[[0.01728477, 0.088313386, -0.028295077, -0.12...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
              "      <td>0</td>\n",
              "      <td>yelp_labelled</td>\n",
              "      <td>Then, wasted enough life there, poured salt wo...</td>\n",
              "      <td>[[0.020121753, 0.21565683, -0.023613028, -0.05...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  ...                                         embeddings\n",
              "2995  I think food should have flavor and texture an...  ...  [[-0.16743083, 0.19883184, 0.049234163, -0.275...\n",
              "2996                           Appetite instantly gone.  ...  [[-0.07648475, 0.191955, -0.057646506, -0.2006...\n",
              "2997  Overall I was not impressed and would not go b...  ...  [[0.0021266676, 0.29672068, -0.16125, -0.01485...\n",
              "2998  The whole experience was underwhelming, and I ...  ...  [[0.01728477, 0.088313386, -0.028295077, -0.12...\n",
              "2999  Then, as if I hadn't wasted enough of my life ...  ...  [[0.020121753, 0.21565683, -0.023613028, -0.05...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rAWESjqEh6UE",
        "outputId": "e2997215-c0a5-4cf7-e5ed-23e522d6bb43"
      },
      "source": [
        "def remove_stop_words(text):\r\n",
        "  words = []\r\n",
        "  for t in text.split():\r\n",
        "    if t.lower() not in stopwordsenglish:\r\n",
        "      words.append(t)\r\n",
        "  return \" \".join(words)\r\n",
        "remove_stop_words(\"You are better when I am well.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'better well.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvbV3jifibMt"
      },
      "source": [
        "df_source['no_stops'] = df_source['sentence'].apply(remove_stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zP6cGlxii_1"
      },
      "source": [
        "# df_source['embeddings'] = df_source['no_stops'].apply(lambda x: nlp(x).vector)\r\n",
        "df_source['embeddings'] = df_source['no_stops'].apply(lambda x: nlp(x).vector.reshape(1,300))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb4Yw1nojk-_",
        "outputId": "487a0ecd-048b-4009-963a-61b78af0ce93"
      },
      "source": [
        "type(df_source['embeddings'][0]), len(df_source['embeddings'][0]),df_source['embeddings'][0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray, 1, (1, 300))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJNw09OptOlv"
      },
      "source": [
        "matrix = np.zeros(shape=(3000,300))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dwp4Zb3u-KW"
      },
      "source": [
        "for i in range(len(df_source['embeddings'])):\r\n",
        "  matrix[i] = df_source['embeddings'][i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El8RBNg3jZhz"
      },
      "source": [
        "X = matrix\r\n",
        "y = df_source[\"sentiment\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N64OD6HwjkCf"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQYtTIg8mZWK"
      },
      "source": [
        "lr = LogisticRegression()\r\n",
        "lr_model = lr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X4oJtFEmiOn",
        "outputId": "3e531ad4-e3ba-4ab4-963c-04313521e4b3"
      },
      "source": [
        "lr_model.score(X_train, y_train), lr_model.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8706666666666667, 0.828)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    }
  ]
}